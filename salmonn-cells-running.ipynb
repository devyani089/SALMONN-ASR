{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3256dce6-34ad-4486-ac9c-19bf3be0899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SALMONN'...\n",
      "remote: Enumerating objects: 826, done.\u001b[K\n",
      "remote: Counting objects: 100% (264/264), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 826 (delta 244), reused 221 (delta 221), pack-reused 562 (from 2)\u001b[K\n",
      "Receiving objects: 100% (826/826), 13.32 MiB | 13.92 MiB/s, done.\n",
      "Resolving deltas: 100% (431/431), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the SALMONN repository from GitHub\n",
    "!git clone https://github.com/bytedance/SALMONN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06a3511-9708-42f2-81f0-035999071900",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e3951b7-a276-4feb-85ee-63dc06a0ac26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520dc78a-38b1-49fd-8378-74e8fa54f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchaudio==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio==2.1.0) (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchaudio==2.1.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchaudio==2.1.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.49.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
      "Requirement already satisfied: wave in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed) (0.8.1)\n",
      "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.1.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.10.6)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.67.1)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.570.86)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
      "Requirement already satisfied: sox in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.24.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from sox) (4.12.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.11.0.86)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.3.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate==0.26.0\n",
      "  Using cached accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.26.0) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\n",
      "Using cached accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.4.0\n",
      "    Uninstalling accelerate-1.4.0:\n",
      "      Successfully uninstalled accelerate-1.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "trl 0.15.1 requires accelerate>=0.34.0, but you have accelerate 0.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.26.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!source new/bin/activate\n",
    "!pip install torch==2.1.0\n",
    "# speech audio processing\n",
    "!pip install torchaudio==2.1.0\n",
    "!pip install transformers sentencepiece\n",
    "!pip install librosa wave\n",
    "# for image processing\n",
    "# !pip install opencv-python PIL\n",
    "!pip install deepspeed\n",
    "!pip install huggingface_hub\n",
    "!pip install huggingface\n",
    "!pip install soundfile sox\n",
    "!pip install numpy \n",
    "!pip install opencv-python Pillow\n",
    "# !pip install peft\n",
    "!pip install accelerate==0.26.0\n",
    "!pip install trl bitsandbytes peft datasets -qU\n",
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca80ed-44a0-4a33-a082-e92a52bd6a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef1b69-2644-4ea5-9715-88c37c3e0b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de0192-4f56-4441-84d0-7d4e4d6385b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e21a9c5-427d-4c90-9527-180eccf7c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a768e5ee-8f6d-4121-b179-ae9625e95c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9681d8ede6a441bfa2e53715d1828250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper model loaded: openai/whisper-large\n",
      "BEATs model loaded: facebook/wav2vec2-large-960h\n",
      "mistral model loaded: mistralai/Mistral-7B-Instruct-v0.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Whisper (ASR model) for speech-to-text\n",
    "whisper_model_name = \"openai/whisper-large\"  # Replace with the desired model name\n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name).to(device)\n",
    "\n",
    "# Load BEATs (for non-speech audio encoding) - example with Wav2Vec2\n",
    "beat_model_name = \"facebook/wav2vec2-large-960h\"  # Replace with BEATs or your preferred audio encoder\n",
    "beat_processor = Wav2Vec2Processor.from_pretrained(beat_model_name)\n",
    "beat_model = Wav2Vec2ForCTC.from_pretrained(beat_model_name).to(device)\n",
    "\n",
    "# # Load Vision Encoder (if applicable)\n",
    "# vision_model_name = \"openai/clip-vit-large-patch14\"  # Replace with vision encoder model if needed\n",
    "# vision_model = AutoModelForCausalLM.from_pretrained(vision_model_name)\n",
    "\n",
    "# alternative to vicuna\n",
    "# Define model name\n",
    "mistral_model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_model_name)\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_model_name,  \n",
    "    torch_dtype=torch.float16 # Optimize RAM usage\n",
    ").to(device)\n",
    "\n",
    "# Print the models' names to confirm they've been loaded\n",
    "print(f\"Whisper model loaded: {whisper_model_name}\")\n",
    "print(f\"BEATs model loaded: {beat_model_name}\")\n",
    "# print(f\"Vision Encoder loaded: {vision_model_name}\")\n",
    "print(f\"mistral model loaded: {mistral_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7679cfd1-36a0-4702-9003-2726a81b6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile numpy torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc1bc7f-a686-4fe8-a8a9-d3f84d3cde0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Audio Transcription Pipeline ===\n",
      "Processing file: Dom.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio from: Dom.wav\n",
      "Resampling audio from 48000Hz to 16000Hz\n",
      "Resampled audio from 48000Hz to 16000Hz\n",
      "Converted to mono: torch.Size([1, 158992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription:\n",
      "--------------------------------------------------\n",
      "Well hey, this is Mark Myocardio, nurse with the Heart to Heart Clinic. Am I speaking with Miss Sarah Rhythm?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import traceback\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "def load_audio(file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"Load audio file and return waveform and sample rate.\"\"\"\n",
    "    try:\n",
    "        waveform, sample_rate = sf.read(file_path)\n",
    "        waveform = torch.from_numpy(waveform)\n",
    "        if len(waveform.shape) == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        elif len(waveform.shape) == 2:\n",
    "            waveform = waveform.T\n",
    "        return waveform.float(), sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio: {e}\")\n",
    "        raise\n",
    "\n",
    "def resample_audio(waveform: torch.Tensor, original_sample_rate: int, target_sample_rate: int = 16000) -> torch.Tensor:\n",
    "    \"\"\"Resample audio tensor to target sample rate.\"\"\"\n",
    "    try:\n",
    "        if original_sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "            print(f\"Resampled audio from {original_sample_rate}Hz to {target_sample_rate}Hz\")\n",
    "        return waveform\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"Exception occurred while resampling - {str(e)}\")\n",
    "        return waveform\n",
    "\n",
    "def resample_audio_np(message: bytes, original_sample_rate: int, new_sample_rate: int = 16000) -> bytes:\n",
    "    \"\"\"Resample audio in numpy array format, updating to target sample rate.\"\"\"\n",
    "    try:\n",
    "        audio_np = np.frombuffer(message, dtype=np.int16)\n",
    "        num_samples = int(len(audio_np) * new_sample_rate / original_sample_rate)\n",
    "        \n",
    "        # This function handles the resampling manually (replace with your resampling logic)\n",
    "        downsampled_audio = resample(audio_np, num_samples)\n",
    "        \n",
    "        # Convert back to byte format\n",
    "        return downsampled_audio.astype(np.int16).tobytes()\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"Exception occurred while converting to proper sampling rate - {str(e)}\")\n",
    "        return message\n",
    "\n",
    "def process_audio(file_path: str) -> Optional[str]:\n",
    "    \"\"\"Process audio file and return transcribed text.\"\"\"\n",
    "    try:\n",
    "        # Load models and processor\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(device)\n",
    "\n",
    "        # Load and preprocess audio\n",
    "        print(f\"Loading audio from: {file_path}\")\n",
    "        waveform, sample_rate = load_audio(file_path)\n",
    "\n",
    "        # Resample audio to 16000Hz (if needed)\n",
    "        if sample_rate != 16000:\n",
    "            print(f\"Resampling audio from {sample_rate}Hz to 16000Hz\")\n",
    "            waveform = resample_audio(waveform, sample_rate, 16000)\n",
    "            sample_rate = 16000  # Update sample_rate after resampling\n",
    "\n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            print(f\"Converted to mono: {waveform.shape}\")\n",
    "\n",
    "        # Normalize audio\n",
    "        waveform = waveform / (torch.max(torch.abs(waveform)) + 1e-8)\n",
    "\n",
    "        # Process with Whisper\n",
    "        input_features = processor(\n",
    "            waveform.numpy().squeeze(),\n",
    "            sampling_rate=sample_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(device)\n",
    "\n",
    "        # Generate transcription\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(\n",
    "            predicted_ids, \n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in audio processing: {e}\")\n",
    "        if 'waveform' in locals():\n",
    "            print(f\"Last waveform shape: {waveform.shape}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # File path\n",
    "    audio_file = \"Dom.wav\"\n",
    "    \n",
    "    print(\"=== Audio Transcription Pipeline ===\")\n",
    "    print(f\"Processing file: {audio_file}\")\n",
    "    \n",
    "    try:\n",
    "        transcription = process_audio(audio_file)\n",
    "        if transcription:\n",
    "            print(\"\\nTranscription:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(transcription)\n",
    "            print(\"-\" * 50)\n",
    "        else:\n",
    "            print(\"Failed to generate transcription\")\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d1ad95-a5bd-41e7-8454-c41a181c836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchAudio version: 2.1.0+cu118\n",
      "Using Wav2Vec2 model for audio embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48167fda62e3496d848f2010b934a076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff048b1abf243259b638af0113b6e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638d1970d77c47609a139b2ce14a8b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Parameter 'function'=<function extract_features at 0x7118ee9b6200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353f36e08c2845b6ba939d9dd4562b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756475e051154268867c10222b2b8eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/73 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First embedding structure: <class 'list'>\n",
      "Audio embedding dimension: 768\n",
      "Aligned audio shape: torch.Size([1, 1, 768])\n",
      "Aligned text shape: torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "# Load dataset\n",
    "audio_dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", split=\"validation\")\n",
    "\n",
    "# Check torchaudio version\n",
    "print(f\"TorchAudio version: {torchaudio.__version__}\")\n",
    "\n",
    "# Use Wav2Vec2 model from HuggingFace\n",
    "print(\"Using Wav2Vec2 model for audio embeddings\")\n",
    "\n",
    "# Load Wav2Vec2 model from HuggingFace\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = AutoModel.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define feature extraction for Wav2Vec2\n",
    "def extract_features(batch):\n",
    "    # Resample if needed (wav2vec2 expects 16kHz)\n",
    "    waveform = torch.tensor(batch[\"audio\"][\"array\"])\n",
    "    sample_rate = batch[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Extract features using Wav2Vec2\n",
    "    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use mean pooling to get a fixed-size representation\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "    # Convert to NumPy array explicitly\n",
    "    embedding_np = embeddings.cpu().numpy()\n",
    "    return {\"embedding\": embedding_np}\n",
    "\n",
    "# Apply feature extraction\n",
    "audio_dataset = audio_dataset.map(extract_features, remove_columns=[\"audio\"])\n",
    "\n",
    "# Save extracted embeddings\n",
    "audio_dataset.save_to_disk(\"audio_embeddings\")\n",
    "\n",
    "# Phase 4: Implement the Q-Former for Multimodal Alignment\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Define Q-Former (inspired by BLIP-2)\n",
    "class QFormer(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=768, audio_dim=768):\n",
    "        super().__init__()\n",
    "        # Be flexible about input dimensions since we might use different audio models\n",
    "        self.audio_proj = torch.nn.Linear(audio_dim, hidden_dim)  # Project audio features\n",
    "        self.text_encoder = BertModel(BertConfig())  # Transformer encoder\n",
    "        \n",
    "    def forward(self, audio_features, text_tokens):\n",
    "        audio_embeddings = self.audio_proj(audio_features)  # Align audio to text space\n",
    "        encoded_text = self.text_encoder(**text_tokens).last_hidden_state\n",
    "        return audio_embeddings, encoded_text  # Return aligned features\n",
    "\n",
    "# Debug: Examine the structure of the first embedding\n",
    "print(\"First embedding structure:\", type(audio_dataset[0][\"embedding\"]))\n",
    "if isinstance(audio_dataset[0][\"embedding\"], list):\n",
    "    # Convert list to numpy array\n",
    "    example_embedding = np.array(audio_dataset[0][\"embedding\"])\n",
    "else:\n",
    "    example_embedding = audio_dataset[0][\"embedding\"]\n",
    "\n",
    "# Get embedding dimension\n",
    "audio_dim = example_embedding.shape[-1]\n",
    "print(f\"Audio embedding dimension: {audio_dim}\")\n",
    "\n",
    "# Instantiate model with the correct input dimension\n",
    "qformer = QFormer(audio_dim=audio_dim)\n",
    "\n",
    "# Example alignment\n",
    "example_audio = torch.tensor(example_embedding).float().unsqueeze(0)  # Ensure float tensor\n",
    "example_text = {\"input_ids\": torch.randint(0, 30522, (1, 10)), \"attention_mask\": torch.ones((1, 10))}\n",
    "\n",
    "# Forward pass\n",
    "aligned_audio, aligned_text = qformer(example_audio, example_text)\n",
    "print(f\"Aligned audio shape: {aligned_audio.shape}\")\n",
    "print(f\"Aligned text shape: {aligned_text.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f246d97-a600-404f-bd8c-a64942347a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ed2ac992984ede91ff8f8eaee87f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717030730c0a4481ae053a8edb728c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c7d92f1fa64fc587cc6f1317d6163f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ed21759c004fecbc71863029b2055b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 1, Loss: 0.029279570261093034\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 2, Loss: 0.005363257678404247\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 3, Loss: 0.004296421800574211\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 4, Loss: 0.0037900266582018708\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 5, Loss: 0.003323921908254493\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer, BertConfig, BertModel\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load precomputed audio embeddings\n",
    "audio_dataset = load_from_disk(\"audio_embeddings\")\n",
    "\n",
    "# Load tokenizer for text processing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Define Q-Former with contrastive loss\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, audio_dim=768):\n",
    "        super().__init__()\n",
    "        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n",
    "        self.text_encoder = BertModel(BertConfig())\n",
    "        self.loss_fn = nn.CosineEmbeddingLoss()\n",
    "    \n",
    "    def forward(self, audio_features, text_tokens, labels):\n",
    "        # Project audio features\n",
    "        audio_embeddings = self.audio_proj(audio_features)\n",
    "        \n",
    "        # Get text embeddings and ensure correct shape\n",
    "        text_outputs = self.text_encoder(**text_tokens)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "            # Print shapes for debugging\n",
    "        print(f\"Audio embeddings shape: {audio_embeddings.shape}\")\n",
    "        print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "        # Make sure both have the same shape: [batch_size, hidden_dim]\n",
    "        if len(audio_embeddings.shape) != len(text_embeddings.shape):\n",
    "            if len(audio_embeddings.shape) > len(text_embeddings.shape):\n",
    "                audio_embeddings = audio_embeddings.squeeze(1)\n",
    "            else:\n",
    "                text_embeddings = text_embeddings.squeeze(1)\n",
    "                \n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn(audio_embeddings, text_embeddings, labels)\n",
    "        return loss, audio_embeddings, text_embeddings\n",
    "\n",
    "# Prepare training data\n",
    "def collate_fn(batch):\n",
    "    audio_inputs = torch.tensor([item[\"embedding\"] for item in batch]).float()\n",
    "    texts = [\"This is a dummy text for contrastive learning\" for _ in batch]  # Placeholder text\n",
    "    text_tokens = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    labels = torch.ones(audio_inputs.shape[0])\n",
    "    return audio_inputs, text_tokens, labels\n",
    "\n",
    "# Initialize model\n",
    "audio_dim = torch.tensor(audio_dataset[0][\"embedding\"]).shape[-1]\n",
    "qformer = QFormer(audio_dim=audio_dim).cuda()\n",
    "optimizer = optim.AdamW(qformer.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train_qformer(epochs=5, batch_size=8):\n",
    "    qformer.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(audio_dataset), batch_size):\n",
    "            batch = [audio_dataset[j] for j in range(i, min(i + batch_size, len(audio_dataset)))]\n",
    "            audio_inputs, text_tokens, labels = collate_fn(batch)\n",
    "            \n",
    "            audio_inputs, labels = audio_inputs.cuda(), labels.cuda()\n",
    "            text_tokens = {k: v.cuda() for k, v in text_tokens.items()}\n",
    "            optimizer.zero_grad()\n",
    "            loss, audio_emb, text_emb = qformer(audio_inputs, text_tokens, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(audio_dataset)}\")\n",
    "\n",
    "# Start training\n",
    "train_qformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9436be8f-73a2-4d4e-b1c6-7092e9754cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: auto-gptq in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.4.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (3.3.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.24.1)\n",
      "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.1)\n",
      "Requirement already satisfied: gekko in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.2.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.1.0+cu118)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.5.2)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.49.0)\n",
      "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.29.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.21.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.11.12)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "464ed303-10d4-4734-8fab-090744a80716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabeae997d2644fca2fef03023bfb86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2900/2339299220.py:201: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b775cfb9b24bb799a832050a37fdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3c226d02a047868035b25697a1884f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4402a6b8d1429aaeb827d8d904322a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bd320cd5e640f494aef5e5328937ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-24 05:59:34,929] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.642400</td>\n",
       "      <td>1.577695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.485400</td>\n",
       "      <td>1.482384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.534000</td>\n",
       "      <td>1.465357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.399400</td>\n",
       "      <td>1.452124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.462700</td>\n",
       "      <td>1.426021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 146.17 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9541670e96ee45c3916a32014b63af62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ab7d8fca4247a3a85474631ef5a567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b7e324cde04f39931e91f554b08424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the model with a sample prompt:\n",
      "--------------------------------------------------\n",
      "### Instruction:\n",
      "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
      "\n",
      "### Input:\n",
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
      "\n",
      "### Response:\n",
      "What are the most common types of grass?\n",
      "\n",
      "I'm looking for something to plant in my front yard. I want it to be easy to take care of, and I want it to be soft to the touch. I also want it to be green and shiny. I'm looking for a grass that grows quickly. I want to plant it in a few days. What's the best grass for me to plant? What are the most common types of grass?\n",
      "\n",
      "I'm looking for a grass that grows quickly and is soft to the touch. I want to plant it in a few days. I want it to be green and shiny. I want to plant it in my front yard. What are the most common types of grass? What's the best grass for me to plant?\n",
      "\n",
      "I'm looking for a grass that grows quickly and is soft to the touch. I want to plant it in my front yard. I want it to be green and shiny. I want to plant it in a few days. What are the most common types of grass? What's the best grass for me to plant?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from datasets import load_dataset\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from transformers import TrainingArguments\n",
    "# from trl import SFTTrainer\n",
    "# from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "\n",
    "# # Load and filter dataset\n",
    "# instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")\n",
    "# instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "# instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(3000))\n",
    "# instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200))\n",
    "\n",
    "# def create_prompt(sample):\n",
    "#     bos_token = \"<s>\"\n",
    "#     original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "#     system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
    "#     response = sample[\"prompt\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "#     input = sample[\"response\"]\n",
    "#     eos_token = \"</s>\"\n",
    "\n",
    "#     full_prompt = f\"{bos_token}### Instruction:\\n{system_message}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{response}{eos_token}\"\n",
    "    \n",
    "#     return full_prompt\n",
    "\n",
    "# # Setup model with proper quantization\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# nf4_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "#     quantization_config=nf4_config,\n",
    "#     use_cache=False\n",
    "# ).to(device)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", use_fast=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# # Configure LoRA\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=64,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, peft_config)\n",
    "\n",
    "# # Training arguments\n",
    "# args = TrainingArguments(\n",
    "#   output_dir=\"mistral_instruct_generation\",\n",
    "#   max_steps=100, \n",
    "#   per_device_train_batch_size=4,\n",
    "#   warmup_steps=0,\n",
    "#   logging_steps=10,\n",
    "#   save_strategy=\"epoch\",\n",
    "#   eval_strategy=\"steps\",  # Fixed parameter name\n",
    "#   eval_steps=20,\n",
    "#   learning_rate=2e-4,\n",
    "#   bf16=True,\n",
    "#   lr_scheduler_type='constant',\n",
    "# )\n",
    "\n",
    "# # Initialize trainer correctly\n",
    "# trainer = SFTTrainer(\n",
    "#   model=model,\n",
    "#   args=args,\n",
    "#   tokenizer=tokenizer,  # Use tokenizer properly\n",
    "#   train_dataset=instruct_tune_dataset[\"train\"],\n",
    "#   eval_dataset=instruct_tune_dataset[\"test\"],\n",
    "#   formatting_func=create_prompt,\n",
    "# )\n",
    "# max_seq_length=1024\n",
    "\n",
    "# # Train the model\n",
    "# import time\n",
    "# start = time.time()\n",
    "# trainer.train()\n",
    "# print(f\"Training completed in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# # Save the model\n",
    "# trainer.save_model(\"mistral_instruct_generation\")\n",
    "# trainer.push_to_hub(\"devyani089/mistral-instruct-generation\")\n",
    "# print(\"model pushed to huggingface\")\n",
    "\n",
    "# # Define a function to generate responses\n",
    "# def generate_response(prompt, model):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "#     outputs = model.generate(\n",
    "#         inputs.input_ids,\n",
    "#         max_new_tokens=512,\n",
    "#         temperature=0.7,\n",
    "#         top_p=0.9,\n",
    "#         do_sample=True\n",
    "#     )\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # Test the model\n",
    "# test_prompt = \"\"\"### Instruction:\n",
    "# Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "# ### Input:\n",
    "# There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
    "\n",
    "# ### Response:\"\"\"\n",
    "\n",
    "# print(generate_response(test_prompt, model))\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "\n",
    "# Load and filter dataset\n",
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")\n",
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(3000))\n",
    "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200))\n",
    "\n",
    "def create_prompt(sample):\n",
    "    bos_token = \"<s>\"\n",
    "    original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
    "    response = sample[\"prompt\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "    input = sample[\"response\"]\n",
    "    eos_token = \"</s>\"\n",
    "\n",
    "    full_prompt = f\"{bos_token}### Instruction:\\n{system_message}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{response}{eos_token}\"\n",
    "    \n",
    "    # Ensure we don't exceed max length\n",
    "    return full_prompt\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup quantization configuration\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False\n",
    ").to(device)\n",
    "\n",
    "# Configure tokenizer properly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Make sure model knows about the pad token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mistral_instruct_generation\",\n",
    "    max_steps=100, \n",
    "    per_device_train_batch_size=4,\n",
    "    warmup_steps=0,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type='constant',\n",
    ")\n",
    "\n",
    "# Initialize trainer with properly configured tokenizer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,  # Pass the actual tokenizer instance\n",
    "    train_dataset=instruct_tune_dataset[\"train\"],\n",
    "    eval_dataset=instruct_tune_dataset[\"test\"],\n",
    "    formatting_func=create_prompt,\n",
    "    # packing=False  # Avoid token mapping issues\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "import time\n",
    "start = time.time()\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(f\"Training completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model(\"mistral_instruct_generation\")\n",
    "    trainer.push_to_hub(\"devyani089/mistral-instruct-generation\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {str(e)}\")\n",
    "\n",
    "# Define a function to generate responses with proper attention masking\n",
    "def generate_response(prompt, model):\n",
    "    # Create inputs with explicit attention mask\n",
    "    encoded_inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    max_seq_length=1024\n",
    "    max_length=1024\n",
    "    # Move everything to the right device\n",
    "    inputs = {k: v.to(model.device) for k, v in encoded_inputs.items()}\n",
    "    \n",
    "    # Generate with explicit attention mask\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the model if training completes successfully\n",
    "if 'trainer' in locals() and hasattr(trainer, 'model'):\n",
    "    test_prompt = \"\"\"### Instruction:\n",
    "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "### Input:\n",
    "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "    print(\"\\nTesting the model with a sample prompt:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(generate_response(test_prompt, model))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbef81c-7b8c-46b7-8f60-e68a1cd081bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a70cbbe66d4d0386c4a7fcdc1c17c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "# Ensure the tokenizer has a padding token\n",
    "if mistral_tokenizer.pad_token is None:\n",
    "    mistral_tokenizer.pad_token = mistral_tokenizer.eos_token  # Set padding token as EOS token\n",
    "\n",
    "lora_model = AutoModelForCausalLM.from_pretrained(\"mistral_instruct_generation\")\n",
    "\n",
    "# Audio feature extraction functions\n",
    "def extract_speech_features(audio_path):\n",
    "    \"\"\"Extract speech features like ASR transcription or linguistic features\"\"\"\n",
    "    # Load audio file\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Example: Using a simple feature extractor (in practice, you might use Whisper or another ASR model)\n",
    "    transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=13\n",
    "    )\n",
    "    mfcc_features = transform(waveform)\n",
    "    \n",
    "    # For this example, we'll return the mean of the features\n",
    "    return torch.mean(mfcc_features, dim=2).flatten()\n",
    "\n",
    "def extract_audio_features(audio_path):\n",
    "    \"\"\"Extract general audio features like volume, pitch, etc.\"\"\"\n",
    "    # Load audio file\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Example: Using spectrogram features\n",
    "    spectrogram = torchaudio.transforms.Spectrogram()(waveform)\n",
    "    \n",
    "    # Return the mean of the spectrogram features\n",
    "    return torch.mean(spectrogram, dim=[1, 2])\n",
    "\n",
    "# Image feature extraction function\n",
    "# Image feature extraction function\n",
    "def extract_image_features(image_path):\n",
    "    \"\"\"Extract visual features from the image\"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Apply transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Use a pre-trained model as feature extractor (example with ResNet)\n",
    "    mistral_model = torchvision.models.resnet18(pretrained=True)\n",
    "    mistral_feature_extractor = torch.nn.Sequential(*list(mistral_model.children())[:-1])\n",
    "    mistral_feature_extractor.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = mistral_feature_extractor(image_tensor)\n",
    "\n",
    "    return features.view(-1)  \n",
    "\n",
    "\n",
    "# Q-Former for feature alignment\n",
    "def q_former_alignment(audio_features, speech_features, image_features):\n",
    "    \"\"\"Align features from different modalities using a Q-Former approach\"\"\"\n",
    "    # This is a simplified placeholder. In practice, you would use a proper \n",
    "    # implementation of a Query Transformer for multimodal alignment\n",
    "    \n",
    "    # Convert features to same dimension if needed\n",
    "    audio_proj = torch.nn.Linear(audio_features.shape[0], 512)(audio_features)\n",
    "    speech_proj = torch.nn.Linear(speech_features.shape[0], 512)(speech_features)\n",
    "    image_proj = torch.nn.Linear(image_features.shape[0], 512)(image_features)\n",
    "    \n",
    "    # Simple concatenation and projection (in a real Q-Former this would be more complex)\n",
    "    concatenated = torch.cat([audio_proj, speech_proj, image_proj])\n",
    "    projection_layer = torch.nn.Linear(3*512, 768)\n",
    "    aligned = projection_layer(concatenated)\n",
    "    \n",
    "    # Convert to string representation for tokenizer\n",
    "    # In practice, you'd have a more sophisticated way to prepare this for the LLM\n",
    "    feature_description = f\"Audio-visual features extracted with values: {aligned[:5].tolist()}\"\n",
    "    \n",
    "    return feature_description\n",
    "\n",
    "# Multimodal processing pipeline\n",
    "def multimodal_processing_pipeline(audio_path, image_path):\n",
    "    # Step 1: Extract features from different modalities\n",
    "    speech_features = extract_speech_features(audio_path)\n",
    "    audio_features = extract_audio_features(audio_path)\n",
    "    image_features = extract_image_features(image_path)\n",
    "    \n",
    "    # Step 2: Align the features using Q-Former\n",
    "    aligned_features = q_former_alignment(audio_features, speech_features, image_features)\n",
    "    \n",
    "    # Step 3: Perform inference using the fine-tuned LoRA model\n",
    "    inputs = mistral_tokenizer(aligned_features, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move to same device as model\n",
    "    device = next(lora_model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output_ids = lora_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=500,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Test the multimodal processing pipeline with an audio and image\n",
    "try:\n",
    "    audio_path = \"Dom.wav\"\n",
    "    image_path = \"clinic.jpeg\"\n",
    "    generated_output = multimodal_processing_pipeline(audio_path, image_path)\n",
    "    print(\"Generated Output:\", generated_output)\n",
    "except Exception as e:\n",
    "    print(f\"Error in pipeline: {str(e)}\")\n",
    "    # Print more detailed traceback\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a565a1-3d43-42d0-937e-6bdd0702fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform task-specific inference\n",
    "def task_specific_inference(audio_path, image_path, task=\"captioning\"):\n",
    "    # Get the multimodal processing result\n",
    "    multimodal_result = multimodal_processing_pipeline(audio_path, image_path)\n",
    "    \n",
    "    if task == \"captioning\":\n",
    "        # For image captioning, assume the output is already a caption\n",
    "        caption = multimodal_result\n",
    "        return caption\n",
    "    \n",
    "    elif task == \"transcription\":\n",
    "        # For transcription, we can directly take the speech processing result\n",
    "        transcription = multimodal_result\n",
    "        return transcription\n",
    "    \n",
    "    elif task == \"question_answering\":\n",
    "        # For question answering, you might provide a question and context\n",
    "        # Example: \"What is in the image?\"\n",
    "        question = \"What is in the image?\"\n",
    "        context = multimodal_result  # Context from multimodal pipeline\n",
    "        qa_input = tokenizer(question + context, return_tensors=\"pt\")\n",
    "        output = lora_model.generate(qa_input.input_ids)\n",
    "        answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "\n",
    "# Example task-specific inference\n",
    "task = \"transcription\" \n",
    "task_result = task_specific_inference(audio_path, image_path, task)\n",
    "\n",
    "print(f\"Task result ({task}):\", task_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e36f8c-dd9a-4232-b94d-72e188defa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize performance for efficiency\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Example training configuration for efficient training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Output directory for model checkpoints\n",
    "    evaluation_strategy=\"steps\",         # Evaluation strategy during training\n",
    "    save_steps=500,                      # Save checkpoint every 500 steps\n",
    "    logging_steps=10,                    # Log every 10 steps\n",
    "    per_device_train_batch_size=4,       # Batch size for training\n",
    "    per_device_eval_batch_size=4,        # Batch size for evaluation\n",
    "    num_train_epochs=3,                  # Number of training epochs\n",
    "    warmup_steps=200,                    # Warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                   # Weight decay for regularization\n",
    "    fp16=True,                           # Use mixed-precision for faster training\n",
    "    gradient_accumulation_steps=2        # Accumulate gradients over 2 steps\n",
    ")\n",
    "\n",
    "# Efficient Inference by batching\n",
    "def batch_inference(input_data, batch_size=4):\n",
    "    # Assuming input_data is a list of multimodal inputs (audio, image pairs)\n",
    "    batched_data = [input_data[i:i+batch_size] for i in range(0, len(input_data), batch_size)]\n",
    "    \n",
    "    results = []\n",
    "    for batch in batched_data:\n",
    "        batch_results = []\n",
    "        for audio_path, image_path in batch:\n",
    "            result = multimodal_processing_pipeline(audio_path, image_path)\n",
    "            batch_results.append(result)\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example of batching inference\n",
    "input_data = [(audio_path, image_path)] * 10  # Simulating 10 pairs\n",
    "inference_results = batch_inference(input_data)\n",
    "\n",
    "print(\"Batch Inference Results:\", inference_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
