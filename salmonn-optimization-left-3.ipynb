{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3256dce6-34ad-4486-ac9c-19bf3be0899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SALMONN'...\n",
      "remote: Enumerating objects: 826, done.\u001b[K\n",
      "remote: Counting objects: 100% (264/264), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 826 (delta 244), reused 221 (delta 221), pack-reused 562 (from 2)\u001b[K\n",
      "Receiving objects: 100% (826/826), 13.32 MiB | 13.92 MiB/s, done.\n",
      "Resolving deltas: 100% (431/431), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the SALMONN repository from GitHub\n",
    "!git clone https://github.com/bytedance/SALMONN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06a3511-9708-42f2-81f0-035999071900",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e3951b7-a276-4feb-85ee-63dc06a0ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source new/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520dc78a-38b1-49fd-8378-74e8fa54f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchaudio==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio==2.1.0) (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchaudio==2.1.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchaudio==2.1.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers)\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m215.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m160.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m166.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m164.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2025.2.0 huggingface-hub-0.29.1 regex-2024.11.6 safetensors-0.5.2 sentencepiece-0.2.0 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.49.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting wave\n",
      "  Downloading Wave-0.0.2.zip (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.1)\n",
      "Collecting scipy>=1.2.0 (from librosa)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=0.20.0 (from librosa)\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting joblib>=0.14 (from librosa)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.0)\n",
      "Collecting lazy-loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (23.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.20.0->librosa)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2022.12.7)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m131.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m220.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m140.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: wave\n",
      "  Building wheel for wave (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wave: filename=Wave-0.0.2-py3-none-any.whl size=1221 sha256=44e561b261373ca5b969101d7a3894bba72588c881114470e5f070439fac903f\n",
      "  Stored in directory: /root/.cache/pip/wheels/f8/24/4d/1b01c0e32da3eb3fd71bbbc6093fcc557ec3b2d9e532ecd65d\n",
      "Successfully built wave\n",
      "Installing collected packages: wave, threadpoolctl, soxr, scipy, msgpack, llvmlite, lazy-loader, joblib, audioread, soundfile, scikit-learn, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.1 joblib-1.4.2 lazy-loader-0.4 librosa-0.10.2.post1 llvmlite-0.44.0 msgpack-1.1.0 numba-0.61.0 pooch-1.8.2 scikit-learn-1.6.1 scipy-1.15.2 soundfile-0.13.1 soxr-0.5.0.post1 threadpoolctl-3.5.0 wave-0.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.16.3.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops (from deepspeed)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.1.0)\n",
      "Collecting ninja (from deepspeed)\n",
      "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.6)\n",
      "Collecting py-cpuinfo (from deepspeed)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic>=2.0.0 (from deepspeed)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.67.1)\n",
      "Collecting nvidia-ml-py (from deepspeed)\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->deepspeed)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.0.0->deepspeed)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic>=2.0.0->deepspeed)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2025.2.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.16.3-py3-none-any.whl size=1550055 sha256=3c608591ef7253c616ace430eec4b1aefcb0ab0562e974cc43c7bbd5b2c1736a\n",
      "  Stored in directory: /root/.cache/pip/wheels/ca/e2/8f/3a91068b57481b104c9c450a20239ec874f6141f8b3769e0dd\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, typing-extensions, ninja, einops, annotated-types, pydantic-core, pydantic, deepspeed\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed annotated-types-0.7.0 deepspeed-0.16.3 einops-0.8.1 hjson-3.1.0 ninja-1.11.1.3 nvidia-ml-py-12.570.86 py-cpuinfo-9.0.0 pydantic-2.10.6 pydantic-core-2.27.2 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting huggingface\n",
      "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: huggingface\n",
      "Successfully installed huggingface-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
      "Collecting sox\n",
      "  Downloading sox-1.5.0.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.24.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from sox) (4.12.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Building wheels for collected packages: sox\n",
      "  Building wheel for sox (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sox: filename=sox-1.5.0-py3-none-any.whl size=40038 sha256=0476043976494efb4e8ef8275e0ab6c394b726d2064ad19d4986fd0dc59029aa\n",
      "  Stored in directory: /root/.cache/pip/wheels/74/e7/7b/8033be3ec5e4994595d01269fc9657c8fd83a0dcbf8536666a\n",
      "Successfully built sox\n",
      "Installing collected packages: sox\n",
      "Successfully installed sox-1.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.3.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.24.1)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m140.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate==0.26.0\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (2025.2.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.0) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.26.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.26.0) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.26.0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\n",
      "Downloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.7/270.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.26.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.0\n",
    "# speech audio processing\n",
    "!pip install torchaudio==2.1.0\n",
    "!pip install transformers sentencepiece\n",
    "!pip install librosa wave\n",
    "# for image processing\n",
    "# !pip install opencv-python PIL\n",
    "!pip install deepspeed\n",
    "!pip install huggingface_hub\n",
    "!pip install huggingface\n",
    "!pip install soundfile sox\n",
    "!pip install numpy \n",
    "!pip install opencv-python Pillow\n",
    "# !pip install peft\n",
    "!pip install accelerate==0.26.0\n",
    "!pip install trl bitsandbytes peft datasets -qU\n",
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e21a9c5-427d-4c90-9527-180eccf7c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a768e5ee-8f6d-4121-b179-ae9625e95c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2265ab1e9b1d48b5977cbc6263cf0362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b44e4f2d2c4434c96e2ec596f3df040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc47d563be94468caa9d4af93b78537d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013b0308b2a746388b005194d8b479a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb6bdb6b5524c2a9619a06725aaade9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8cb27d98eb47a0b4e7f9a3fe51be6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5c7560255249f0a4ffde7c1df20448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2bd078df3d41a3aec8ffcbc54906d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf510f64f9c4488b4a8ba8c1b231cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203fda22619e41a1a942f0ab9f10ecc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6f7528067e4963a8f6d884f1b3c43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609e267244014e578d54ea069e650bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420e7709df6848f3a62f030b876926c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7949a27789e34240a08df657a63722f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac652948cfb448a8982d55b53aeef9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3630af9336f4c9ca256c8a43dd05013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a2807395494b8da372f174e97ed147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eae8b474a034b119f75aaa2c0c0dd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2b2caa8c914ee0ae1e5216245bb4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2faff15f95540bf8f7b2dc6e88ba46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c21e49ea4654d699c581bbd60889960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368704f73db14402aa77e18d3c66fc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d215fcf6025412fa83180196a18da85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2231883ff9e44abfb30b2865102b8df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c025aa52c214e76a1c859a3531abf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817a6315a7f74d358fe70eb8dadb3ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59c72edfe744538894938e99f8f96fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d267c01e994493894043306c88fd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d95497ec214f47b908e9de6544ce46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper model loaded: openai/whisper-large\n",
      "BEATs model loaded: facebook/wav2vec2-large-960h\n",
      "mistral model loaded: mistralai/Mistral-7B-Instruct-v0.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Whisper (ASR model) for speech-to-text\n",
    "whisper_model_name = \"openai/whisper-large\" \n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name).to(device)\n",
    "\n",
    "# Load BEATs (for non-speech audio encoding) - example with Wav2Vec2\n",
    "beat_model_name = \"facebook/wav2vec2-large-960h\"  #replaced with Wav2Vec2, BEAT was not loading\n",
    "beat_processor = Wav2Vec2Processor.from_pretrained(beat_model_name)\n",
    "beat_model = Wav2Vec2ForCTC.from_pretrained(beat_model_name).to(device)\n",
    "\n",
    "# # Load Vision Encoder (if applicable)\n",
    "# vision_model_name = \"openai/clip-vit-large-patch14\"  # Replace with vision encoder model if needed\n",
    "# vision_model = AutoModelForCausalLM.from_pretrained(vision_model_name)\n",
    "\n",
    "# alternative to vicuna\n",
    "# Define model name\n",
    "mistral_model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_model_name, use_fast=True)\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_model_name,  \n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False\n",
    ").to(device)\n",
    "\n",
    "# Print the models' names to confirm they've been loaded\n",
    "print(f\"Whisper model loaded: {whisper_model_name}\")\n",
    "print(f\"BEATs model loaded: {beat_model_name}\")\n",
    "# print(f\"Vision Encoder loaded: {vision_model_name}\")\n",
    "print(f\"mistral model loaded: {mistral_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7679cfd1-36a0-4702-9003-2726a81b6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile numpy torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc1bc7f-a686-4fe8-a8a9-d3f84d3cde0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Audio Transcription Pipeline ===\n",
      "Processing file: Dom.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio from: Dom.wav\n",
      "Resampling audio from 48000Hz to 16000Hz\n",
      "Resampled audio from 48000Hz to 16000Hz\n",
      "Converted to mono: torch.Size([1, 158992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription:\n",
      "--------------------------------------------------\n",
      "Well hey, this is Mark Myocardio, nurse with the Heart to Heart Clinic. Am I speaking with Miss Sarah Rhythm?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import traceback\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "def load_audio(file_path: str) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"Load audio file and return waveform and sample rate.\"\"\"\n",
    "    try:\n",
    "        waveform, sample_rate = sf.read(file_path)\n",
    "        waveform = torch.from_numpy(waveform)\n",
    "        if len(waveform.shape) == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        elif len(waveform.shape) == 2:\n",
    "            waveform = waveform.T\n",
    "        return waveform.float(), sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio: {e}\")\n",
    "        raise\n",
    "\n",
    "def resample_audio(waveform: torch.Tensor, original_sample_rate: int, target_sample_rate: int = 16000) -> torch.Tensor:\n",
    "    \"\"\"Resample audio tensor to target sample rate.\"\"\"\n",
    "    try:\n",
    "        if original_sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "            print(f\"Resampled audio from {original_sample_rate}Hz to {target_sample_rate}Hz\")\n",
    "        return waveform\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"Exception occurred while resampling - {str(e)}\")\n",
    "        return waveform\n",
    "\n",
    "def resample_audio_np(message: bytes, original_sample_rate: int, new_sample_rate: int = 16000) -> bytes:\n",
    "    \"\"\"Resample audio in numpy array format, updating to target sample rate.\"\"\"\n",
    "    try:\n",
    "        audio_np = np.frombuffer(message, dtype=np.int16)\n",
    "        num_samples = int(len(audio_np) * new_sample_rate / original_sample_rate)\n",
    "        \n",
    "        # This function handles the resampling manually (replace with your resampling logic)\n",
    "        downsampled_audio = resample(audio_np, num_samples)\n",
    "        \n",
    "        # Convert back to byte format\n",
    "        return downsampled_audio.astype(np.int16).tobytes()\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"Exception occurred while converting to proper sampling rate - {str(e)}\")\n",
    "        return message\n",
    "\n",
    "def process_audio(file_path: str) -> Optional[str]:\n",
    "    \"\"\"Process audio file and return transcribed text.\"\"\"\n",
    "    try:\n",
    "        # # Load models and processor\n",
    "        # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "        # model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\").to(device)\n",
    "\n",
    "        # Load and preprocess audio\n",
    "        print(f\"Loading audio from: {file_path}\")\n",
    "        waveform, sample_rate = load_audio(file_path)\n",
    "\n",
    "        # Resample audio to 16000Hz (if needed)\n",
    "        if sample_rate != 16000:\n",
    "            print(f\"Resampling audio from {sample_rate}Hz to 16000Hz\")\n",
    "            waveform = resample_audio(waveform, sample_rate, 16000)\n",
    "            sample_rate = 16000  # Update sample_rate after resampling\n",
    "\n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            print(f\"Converted to mono: {waveform.shape}\")\n",
    "\n",
    "        # Normalize audio\n",
    "        waveform = waveform / (torch.max(torch.abs(waveform)) + 1e-8)\n",
    "\n",
    "        # Process with Whisper\n",
    "        input_features = whisper_processor(\n",
    "            waveform.numpy().squeeze(),\n",
    "            sampling_rate=sample_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(device)\n",
    "\n",
    "        # Generate transcription\n",
    "        predicted_ids = whisper_model.generate(input_features)\n",
    "        transcription = whisper_processor.batch_decode(\n",
    "            predicted_ids, \n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in audio processing: {e}\")\n",
    "        if 'waveform' in locals():\n",
    "            print(f\"Last waveform shape: {waveform.shape}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # File path\n",
    "    audio_file = \"Dom.wav\"\n",
    "    \n",
    "    print(\"=== Audio Transcription Pipeline ===\")\n",
    "    print(f\"Processing file: {audio_file}\")\n",
    "    \n",
    "    try:\n",
    "        transcription = process_audio(audio_file)\n",
    "        if transcription:\n",
    "            print(\"\\nTranscription:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(transcription)\n",
    "            print(\"-\" * 50)\n",
    "        else:\n",
    "            print(\"Failed to generate transcription\")\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26d1ad95-a5bd-41e7-8454-c41a181c836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchAudio version: 2.1.0+cu118\n",
      "Using Wav2Vec2 model for audio embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc676faf6c241988af0f99636df5d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1552e864b50c4cc5bf58980a1b1942eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/73 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First embedding structure: <class 'list'>\n",
      "Audio embedding dimension: 768\n",
      "Aligned audio shape: torch.Size([1, 1, 768])\n",
      "Aligned text shape: torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "# Load dataset\n",
    "audio_dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", split=\"validation\")\n",
    "\n",
    "# Check torchaudio version\n",
    "print(f\"TorchAudio version: {torchaudio.__version__}\")\n",
    "\n",
    "# Use Wav2Vec2 model from HuggingFace\n",
    "print(\"Using Wav2Vec2 model for audio embeddings\")\n",
    "\n",
    "# Load Wav2Vec2 model from HuggingFace\n",
    "beat_feature_extractor = AutoFeatureExtractor.from_pretrained(beat_model_name)\n",
    "# model = AutoModel.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define feature extraction for Wav2Vec2\n",
    "def extract_features(batch):\n",
    "    # Resample if needed (wav2vec2 expects 16kHz)\n",
    "    waveform = torch.tensor(batch[\"audio\"][\"array\"])\n",
    "    sample_rate = batch[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Extract features using Wav2Vec2\n",
    "    inputs = beat_feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = beat_model(**inputs)\n",
    "        # Use mean pooling to get a fixed-size representation\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "    # Convert to NumPy array explicitly\n",
    "    embedding_np = embeddings.cpu().numpy()\n",
    "    return {\"embedding\": embedding_np}\n",
    "\n",
    "# Apply feature extraction\n",
    "audio_dataset = audio_dataset.map(extract_features, remove_columns=[\"audio\"])\n",
    "\n",
    "# Save extracted embeddings\n",
    "audio_dataset.save_to_disk(\"audio_embeddings\")\n",
    "\n",
    "# Phase 4: Implement the Q-Former for Multimodal Alignment\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Define Q-Former (inspired by BLIP-2)\n",
    "class QFormer(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=768, audio_dim=768):\n",
    "        super().__init__()\n",
    "        # Be flexible about input dimensions since we might use different audio models\n",
    "        self.audio_proj = torch.nn.Linear(audio_dim, hidden_dim)  # Project audio features\n",
    "        self.text_encoder = BertModel(BertConfig())  # Transformer encoder\n",
    "        \n",
    "    def forward(self, audio_features, text_tokens):\n",
    "        audio_embeddings = self.audio_proj(audio_features)  # Align audio to text space\n",
    "        encoded_text = self.text_encoder(**text_tokens).last_hidden_state\n",
    "        return audio_embeddings, encoded_text  # Return aligned features\n",
    "\n",
    "# Debug: Examine the structure of the first embedding\n",
    "print(\"First embedding structure:\", type(audio_dataset[0][\"embedding\"]))\n",
    "if isinstance(audio_dataset[0][\"embedding\"], list):\n",
    "    # Convert list to numpy array\n",
    "    example_embedding = np.array(audio_dataset[0][\"embedding\"])\n",
    "else:\n",
    "    example_embedding = audio_dataset[0][\"embedding\"]\n",
    "\n",
    "# Get embedding dimension\n",
    "audio_dim = example_embedding.shape[-1]\n",
    "print(f\"Audio embedding dimension: {audio_dim}\")\n",
    "\n",
    "# Instantiate model with the correct input dimension\n",
    "qformer = QFormer(audio_dim=audio_dim)\n",
    "\n",
    "# Example alignment\n",
    "example_audio = torch.tensor(example_embedding).float().unsqueeze(0)  # Ensure float tensor\n",
    "example_text = {\"input_ids\": torch.randint(0, 30522, (1, 10)), \"attention_mask\": torch.ones((1, 10))}\n",
    "\n",
    "# Forward pass\n",
    "aligned_audio, aligned_text = qformer(example_audio, example_text)\n",
    "print(f\"Aligned audio shape: {aligned_audio.shape}\")\n",
    "print(f\"Aligned text shape: {aligned_text.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f246d97-a600-404f-bd8c-a64942347a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 1, Loss: 0.029339247676607682\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 2, Loss: 0.005391104580604867\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 3, Loss: 0.004233830799795177\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 4, Loss: 0.0036119513519822736\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([8, 1, 768])\n",
      "Text embeddings shape: torch.Size([8, 768])\n",
      "Audio embeddings shape: torch.Size([1, 1, 768])\n",
      "Text embeddings shape: torch.Size([1, 768])\n",
      "Epoch 5, Loss: 0.0032670588730132743\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer, BertConfig, BertModel\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load precomputed audio embeddings\n",
    "audio_dataset = load_from_disk(\"audio_embeddings\")\n",
    "\n",
    "# Load tokenizer for text processing\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Define Q-Former with contrastive loss\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, audio_dim=768):\n",
    "        super().__init__()\n",
    "        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n",
    "        self.text_encoder = BertModel(BertConfig())\n",
    "        self.loss_fn = nn.CosineEmbeddingLoss()\n",
    "    \n",
    "    def forward(self, audio_features, text_tokens, labels):\n",
    "        # Project audio features\n",
    "        audio_embeddings = self.audio_proj(audio_features)\n",
    "        \n",
    "        # Get text embeddings and ensure correct shape\n",
    "        text_outputs = self.text_encoder(**text_tokens)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "            # Print shapes for debugging\n",
    "        print(f\"Audio embeddings shape: {audio_embeddings.shape}\")\n",
    "        print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "        # Make sure both have the same shape: [batch_size, hidden_dim]\n",
    "        if len(audio_embeddings.shape) != len(text_embeddings.shape):\n",
    "            if len(audio_embeddings.shape) > len(text_embeddings.shape):\n",
    "                audio_embeddings = audio_embeddings.squeeze(1)\n",
    "            else:\n",
    "                text_embeddings = text_embeddings.squeeze(1)\n",
    "                \n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn(audio_embeddings, text_embeddings, labels)\n",
    "        return loss, audio_embeddings, text_embeddings\n",
    "\n",
    "# Prepare training data\n",
    "def collate_fn(batch):\n",
    "    audio_inputs = torch.tensor([item[\"embedding\"] for item in batch]).float()\n",
    "    texts = [\"This is a dummy text for contrastive learning\" for _ in batch]  # Placeholder text\n",
    "    text_tokens = mistral_tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    labels = torch.ones(audio_inputs.shape[0])\n",
    "    return audio_inputs, text_tokens, labels\n",
    "\n",
    "# Initialize model\n",
    "audio_dim = torch.tensor(audio_dataset[0][\"embedding\"]).shape[-1]\n",
    "qformer = QFormer(audio_dim=audio_dim).cuda()\n",
    "optimizer = optim.AdamW(qformer.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train_qformer(epochs=5, batch_size=8):\n",
    "    qformer.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(audio_dataset), batch_size):\n",
    "            batch = [audio_dataset[j] for j in range(i, min(i + batch_size, len(audio_dataset)))]\n",
    "            audio_inputs, text_tokens, labels = collate_fn(batch)\n",
    "            \n",
    "            audio_inputs, labels = audio_inputs.cuda(), labels.cuda()\n",
    "            text_tokens = {k: v.cuda() for k, v in text_tokens.items()}\n",
    "            optimizer.zero_grad()\n",
    "            loss, audio_emb, text_emb = qformer(audio_inputs, text_tokens, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(audio_dataset)}\")\n",
    "\n",
    "# Start training\n",
    "train_qformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9436be8f-73a2-4d4e-b1c6-7092e9754cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-gptq\n",
      "  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.4.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (3.3.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.24.1)\n",
      "Collecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting gekko (from auto-gptq)\n",
      "  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.1.0+cu118)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.5.2)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.49.0)\n",
      "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.21.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.11.12)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge, gekko, auto-gptq\n",
      "Successfully installed auto-gptq-0.7.1 gekko-1.2.1 rouge-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "464ed303-10d4-4734-8fab-090744a80716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bab35f616a745eeb0e1e9677a417e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_347/2696335676.py:201: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5ff0eef85d4c9fb6541bc9c7546d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e262ecf69a044a18c3b4110fea66749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6494173f5e4f415c9790393fd35cedf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1824e47cd10748ae936e3a7bb0a6f97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182167d748694a4caf637b480695abf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a84c23e482942a8b0ea03aa38f69b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffffd6f298541509f7f95f85056481a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2694e60ed741abb4d95375a83a6cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4c4c4109ea4db1aaed38d4e6a25a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d5e18ce1204bb3a993cdf7963bbff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.635400</td>\n",
       "      <td>1.572143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.482700</td>\n",
       "      <td>1.481565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.533600</td>\n",
       "      <td>1.464719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.401700</td>\n",
       "      <td>1.454052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.468500</td>\n",
       "      <td>1.432739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 235.23 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336b5127e9c646e594b16f4efa1e9357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5002ca9b55db4963a5d42df2e643c12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c870ae41fcf434b81aff1096dcc2e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the model with a sample prompt:\n",
      "--------------------------------------------------\n",
      "### Instruction:\n",
      "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
      "\n",
      "### Input:\n",
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
      "\n",
      "### Response:\n",
      "What are some common species of grass?\n",
      "\n",
      "Besides Kentucky Bluegrass, there are many other species of grass.  You may have heard of some of them, like Bermuda grass, Zoysia grass, or ryegrass.  But there are many, many more.  Some of them are soft and fluffy, while others are rough and hard.  Some of them grow quickly and easily, while others grow slowly.  Some of them grow in dry soil, while others grow in wet soil.  Some of them grow in sunny, hot weather, while others grow in cool, shady weather.  There are many, many more.  So there are more than 12,000 species of grass.  That's a lot!\n",
      "\n",
      "In fact, there are so many different species of grass that you may not even know which one you have in your lawn!  Some of the most common ones are Kentucky Bluegrass, Bermuda grass, and ryegrass.  Kentucky Bluegrass is one of the most popular.  It grows quickly and easily.  It's soft and fluffy.  And it's a very good grass for families, because it's easy to grow and care for.\n",
      "\n",
      "Ryegrass is shiny and bright green.  It's also very popular, because it's good for sports and lawns.  It's a good grass for families, because it's easy to grow and care for.\n",
      "\n",
      "Fescues are dark green and shiny.  They're good for families, because they're easy to grow and care for.  They're also good for golf courses, because they're soft and fluffy.\n",
      "\n",
      "Bermuda grass is harder than Kentucky Bluegrass, but it can grow in drier soil.  It's good for families, because it's easy to grow and care for.  It's also good for golf courses, because it's soft and fluffy.\n",
      "\n",
      "So there are many different species of grass, and some of them are soft and fluffy, while others are rough and hard.  Some of them grow quickly and easily, while others grow slowly.  Some of them grow in dry soil, while others grow in wet soil.  Some of them grow in sunny, hot weather, while others grow in cool, shady weather.  There are many,\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from datasets import load_dataset\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from transformers import TrainingArguments\n",
    "# from trl import SFTTrainer\n",
    "# from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "\n",
    "# # Load and filter dataset\n",
    "# instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")\n",
    "# instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "# instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(3000))\n",
    "# instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200))\n",
    "\n",
    "# def create_prompt(sample):\n",
    "#     bos_token = \"<s>\"\n",
    "#     original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "#     system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
    "#     response = sample[\"prompt\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "#     input = sample[\"response\"]\n",
    "#     eos_token = \"</s>\"\n",
    "\n",
    "#     full_prompt = f\"{bos_token}### Instruction:\\n{system_message}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{response}{eos_token}\"\n",
    "    \n",
    "#     return full_prompt\n",
    "\n",
    "# # Setup model with proper quantization\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# nf4_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "#     quantization_config=nf4_config,\n",
    "#     use_cache=False\n",
    "# ).to(device)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", use_fast=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# # Configure LoRA\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=64,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, peft_config)\n",
    "\n",
    "# # Training arguments\n",
    "# args = TrainingArguments(\n",
    "#   output_dir=\"mistral_instruct_generation\",\n",
    "#   max_steps=100, \n",
    "#   per_device_train_batch_size=4,\n",
    "#   warmup_steps=0,\n",
    "#   logging_steps=10,\n",
    "#   save_strategy=\"epoch\",\n",
    "#   eval_strategy=\"steps\",  # Fixed parameter name\n",
    "#   eval_steps=20,\n",
    "#   learning_rate=2e-4,\n",
    "#   bf16=True,\n",
    "#   lr_scheduler_type='constant',\n",
    "# )\n",
    "\n",
    "# # Initialize trainer correctly\n",
    "# trainer = SFTTrainer(\n",
    "#   model=model,\n",
    "#   args=args,\n",
    "#   tokenizer=tokenizer,  # Use tokenizer properly\n",
    "#   train_dataset=instruct_tune_dataset[\"train\"],\n",
    "#   eval_dataset=instruct_tune_dataset[\"test\"],\n",
    "#   formatting_func=create_prompt,\n",
    "# )\n",
    "# max_seq_length=1024\n",
    "\n",
    "# # Train the model\n",
    "# import time\n",
    "# start = time.time()\n",
    "# trainer.train()\n",
    "# print(f\"Training completed in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# # Save the model\n",
    "# trainer.save_model(\"mistral_instruct_generation\")\n",
    "# trainer.push_to_hub(\"devyani089/mistral-instruct-generation\")\n",
    "# print(\"model pushed to huggingface\")\n",
    "\n",
    "# # Define a function to generate responses\n",
    "# def generate_response(prompt, model):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "#     outputs = model.generate(\n",
    "#         inputs.input_ids,\n",
    "#         max_new_tokens=512,\n",
    "#         temperature=0.7,\n",
    "#         top_p=0.9,\n",
    "#         do_sample=True\n",
    "#     )\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # Test the model\n",
    "# test_prompt = \"\"\"### Instruction:\n",
    "# Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "# ### Input:\n",
    "# There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
    "\n",
    "# ### Response:\"\"\"\n",
    "\n",
    "# print(generate_response(test_prompt, model))\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "\n",
    "# Load and filter dataset\n",
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")\n",
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(3000))\n",
    "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200))\n",
    "\n",
    "def create_prompt(sample):\n",
    "    bos_token = \"<s>\"\n",
    "    original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
    "    response = sample[\"prompt\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "    input = sample[\"response\"]\n",
    "    eos_token = \"</s>\"\n",
    "\n",
    "    full_prompt = f\"{bos_token}### Instruction:\\n{system_message}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{response}{eos_token}\"\n",
    "    \n",
    "    # Ensure we don't exceed max length\n",
    "    return full_prompt\n",
    "\n",
    "\n",
    "# Setup quantization configuration\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the model with quantization\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "#     quantization_config=nf4_config,\n",
    "#     use_cache=False\n",
    "# ).to(device)\n",
    "\n",
    "# Configure tokenizer properly\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "    # Make sure model knows about the pad token\n",
    "    model.config.pad_token_id = mistral_tokenizer.pad_token_id\n",
    "mistral_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "mistral_model = prepare_model_for_kbit_training(model)\n",
    "mistral_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mistral_instruct_generation\",\n",
    "    max_steps=100, \n",
    "    per_device_train_batch_size=4,\n",
    "    warmup_steps=0,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type='constant',\n",
    ")\n",
    "\n",
    "# Initialize trainer with properly configured tokenizer\n",
    "trainer = SFTTrainer(\n",
    "    model=mistral_model,\n",
    "    args=args,\n",
    "    tokenizer=mistral_tokenizer,  # Pass the actual tokenizer instance\n",
    "    train_dataset=instruct_tune_dataset[\"train\"],\n",
    "    eval_dataset=instruct_tune_dataset[\"test\"],\n",
    "    formatting_func=create_prompt,\n",
    "    # packing=False  # Avoid token mapping issues\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "import time\n",
    "start = time.time()\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(f\"Training completed in {time.time() - start:.2f} seconds\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model(\"mistral_instruct_generation\")\n",
    "    trainer.push_to_hub(\"devyani089/mistral-instruct-generation\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {str(e)}\")\n",
    "\n",
    "# Define a function to generate responses with proper attention masking\n",
    "def generate_response(prompt, model):\n",
    "    # Create inputs with explicit attention mask\n",
    "    encoded_inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    max_seq_length=1024\n",
    "    max_length=1024\n",
    "    # Move everything to the right device\n",
    "    inputs = {k: v.to(model.device) for k, v in encoded_inputs.items()}\n",
    "    \n",
    "    # Generate with explicit attention mask\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=mistral_tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    return mistral_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the model if training completes successfully\n",
    "if 'trainer' in locals() and hasattr(trainer, 'model'):\n",
    "    test_prompt = \"\"\"### Instruction:\n",
    "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
    "\n",
    "### Input:\n",
    "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "    print(\"\\nTesting the model with a sample prompt:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(generate_response(test_prompt, model))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfbef81c-7b8c-46b7-8f60-e68a1cd081bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8add5636c2e247e7a5359b75914a0a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in pipeline: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_347/2424721171.py\", line 126, in <module>\n",
      "    generated_output = multimodal_processing_pipeline(audio_path, image_path)\n",
      "  File \"/tmp/ipykernel_347/2424721171.py\", line 100, in multimodal_processing_pipeline\n",
      "    inputs = tokenizer(aligned_features, return_tensors=\"pt\", padding=True, truncation=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2877, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2987, in _call_one\n",
      "    return self.encode_plus(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 3054, in encode_plus\n",
      "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2779, in _get_padding_truncation_strategies\n",
      "    raise ValueError(\n",
      "ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "login(token=\"\")\n",
    "\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "lora_model = AutoModelForCausalLM.from_pretrained(\"mistral_instruct_generation\")\n",
    "\n",
    "# Audio feature extraction functions\n",
    "def extract_speech_features(audio_path):\n",
    "    \"\"\"Extract speech features like ASR transcription or linguistic features\"\"\"\n",
    "    # Load audio file\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Example: Using a simple feature extractor (in practice, you might use Whisper or another ASR model)\n",
    "    transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=13\n",
    "    )\n",
    "    mfcc_features = transform(waveform)\n",
    "    \n",
    "    # For this example, we'll return the mean of the features\n",
    "    return torch.mean(mfcc_features, dim=2).flatten()\n",
    "\n",
    "def extract_audio_features(audio_path):\n",
    "    \"\"\"Extract general audio features like volume, pitch, etc.\"\"\"\n",
    "    # Load audio file\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Example: Using spectrogram features\n",
    "    spectrogram = torchaudio.transforms.Spectrogram()(waveform)\n",
    "    \n",
    "    # Return the mean of the spectrogram features\n",
    "    return torch.mean(spectrogram, dim=[1, 2])\n",
    "\n",
    "# Image feature extraction function\n",
    "def extract_image_features(image_path):\n",
    "    \"\"\"Extract visual features from the image\"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Apply transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    \n",
    "    # Use a pre-trained model as feature extractor (example with ResNet)\n",
    "    mistral_model = torchvision.models.resnet18(pretrained=True)\n",
    "    mistral_feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    mistral_feature_extractor.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = mistral_feature_extractor(image_tensor)\n",
    "    \n",
    "    return features.flatten()\n",
    "\n",
    "# Q-Former for feature alignment\n",
    "def q_former_alignment(audio_features, speech_features, image_features):\n",
    "    \"\"\"Align features from different modalities using a Q-Former approach\"\"\"\n",
    "    # This is a simplified placeholder. In practice, you would use a proper \n",
    "    # implementation of a Query Transformer for multimodal alignment\n",
    "    \n",
    "    # Convert features to same dimension if needed\n",
    "    audio_proj = torch.nn.Linear(audio_features.shape[0], 512)(audio_features)\n",
    "    speech_proj = torch.nn.Linear(speech_features.shape[0], 512)(speech_features)\n",
    "    image_proj = torch.nn.Linear(image_features.shape[0], 512)(image_features)\n",
    "    \n",
    "    # Simple concatenation and projection (in a real Q-Former this would be more complex)\n",
    "    concatenated = torch.cat([audio_proj, speech_proj, image_proj])\n",
    "    projection_layer = torch.nn.Linear(3*512, 768)\n",
    "    aligned = projection_layer(concatenated)\n",
    "    \n",
    "    # Convert to string representation for tokenizer\n",
    "    # In practice, you'd have a more sophisticated way to prepare this for the LLM\n",
    "    feature_description = f\"Audio-visual features extracted with values: {aligned[:5].tolist()}\"\n",
    "    \n",
    "    return feature_description\n",
    "\n",
    "# Multimodal processing pipeline\n",
    "def multimodal_processing_pipeline(audio_path, image_path):\n",
    "    # Step 1: Extract features from different modalities\n",
    "    speech_features = extract_speech_features(audio_path)\n",
    "    audio_features = extract_audio_features(audio_path)\n",
    "    image_features = extract_image_features(image_path)\n",
    "    \n",
    "    # Step 2: Align the features using Q-Former\n",
    "    aligned_features = q_former_alignment(audio_features, speech_features, image_features)\n",
    "    \n",
    "    # Step 3: Perform inference using the fine-tuned LoRA model\n",
    "    inputs = tokenizer(aligned_features, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move to same device as model\n",
    "    device = next(lora_model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output_ids = lora_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=50,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = mistral_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Test the multimodal processing pipeline with an audio and image\n",
    "try:\n",
    "    audio_path = \"Dom.wav\"\n",
    "    image_path = \"clinic.jpeg\"\n",
    "    generated_output = multimodal_processing_pipeline(audio_path, image_path)\n",
    "    print(\"Generated Output:\", generated_output)\n",
    "except Exception as e:\n",
    "    print(f\"Error in pipeline: {str(e)}\")\n",
    "    # Print more detailed traceback\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a565a1-3d43-42d0-937e-6bdd0702fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform task-specific inference\n",
    "def task_specific_inference(audio_path, image_path, task=\"captioning\"):\n",
    "    # Get the multimodal processing result\n",
    "    multimodal_result = multimodal_processing_pipeline(audio_path, image_path)\n",
    "    \n",
    "    if task == \"captioning\":\n",
    "        # For image captioning, assume the output is already a caption\n",
    "        caption = multimodal_result\n",
    "        return caption\n",
    "    \n",
    "    elif task == \"transcription\":\n",
    "        # For transcription, we can directly take the speech processing result\n",
    "        transcription = multimodal_result\n",
    "        return transcription\n",
    "    \n",
    "    elif task == \"question_answering\":\n",
    "        # For question answering, you might provide a question and context\n",
    "        # Example: \"What is in the image?\"\n",
    "        question = \"What is in the image?\"\n",
    "        context = multimodal_result  # Context from multimodal pipeline\n",
    "        qa_input = tokenizer(question + context, return_tensors=\"pt\")\n",
    "        output = lora_model.generate(qa_input.input_ids)\n",
    "        answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "\n",
    "# Example task-specific inference\n",
    "task = \"captioning\"  # Change this to \"transcription\" or \"question_answering\" as needed\n",
    "task_result = task_specific_inference(audio_path, image_path, task)\n",
    "\n",
    "print(f\"Task result ({task}):\", task_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e36f8c-dd9a-4232-b94d-72e188defa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize performance for efficiency\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Example training configuration for efficient training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Output directory for model checkpoints\n",
    "    evaluation_strategy=\"steps\",         # Evaluation strategy during training\n",
    "    save_steps=500,                      # Save checkpoint every 500 steps\n",
    "    logging_steps=10,                    # Log every 10 steps\n",
    "    per_device_train_batch_size=4,       # Batch size for training\n",
    "    per_device_eval_batch_size=4,        # Batch size for evaluation\n",
    "    num_train_epochs=3,                  # Number of training epochs\n",
    "    warmup_steps=200,                    # Warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                   # Weight decay for regularization\n",
    "    fp16=True,                           # Use mixed-precision for faster training\n",
    "    gradient_accumulation_steps=2        # Accumulate gradients over 2 steps\n",
    ")\n",
    "\n",
    "# Efficient Inference by batching\n",
    "def batch_inference(input_data, batch_size=4):\n",
    "    # Assuming input_data is a list of multimodal inputs (audio, image pairs)\n",
    "    batched_data = [input_data[i:i+batch_size] for i in range(0, len(input_data), batch_size)]\n",
    "    \n",
    "    results = []\n",
    "    for batch in batched_data:\n",
    "        batch_results = []\n",
    "        for audio_path, image_path in batch:\n",
    "            result = multimodal_processing_pipeline(audio_path, image_path)\n",
    "            batch_results.append(result)\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example of batching inference\n",
    "input_data = [(audio_path, image_path)] * 10  # Simulating 10 pairs\n",
    "inference_results = batch_inference(input_data)\n",
    "\n",
    "print(\"Batch Inference Results:\", inference_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
